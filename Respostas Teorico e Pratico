##################
##Teste teórico

Qual o objetivo do comando cache​ ​em Spark?
R: Permite executar comandos de tranformação e ação em memória.

O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em
MapReduce. Por quê?
R: O processamento do Spark é feito em memória usando as estruturas RDD. Já o MapReduce processa os dados direto no disco rígido.

Qual é a função do SparkContext​?
R: Cria uma sessão da aplicação no Cluster.

Explique com suas palavras o que é Resilient​ ​Distributed​ ​Datasets​ (RDD).
R: O RDD é uma forma de estrutura de dados que o Spark utiliza para processamento.

GroupByKey​ ​é menos eficiente que reduceByKey​ ​em grandes dataset. Por quê?
R: O groupByKey envia uma grande quantidade de dados para os workers de redução.
O reduceByKey combina os dados em cada partição, apenas uma saída para uma chave em cada partição pode ser enviada pela rede.

Explique o que o código Scala abaixo faz.

val textFile = sc.textFile("hdfs://...")
val counts = textFile.flatMap(line => line.split(" "))
.map(word => (word, 1))
.reduceByKey(_ + _)
counts.saveAsTextFile("hdfs://...")

R: O código acima carrega arquivo de texto na variável textFile (RDD).
Transforma o textFile separando as palavras por linha.
Conta cada palavra que foi separada.
Salva o resultado em um arquivo de texto.

##################
##Teste prático

1. Número de hosts únicos.
R:

2. O total de erros 404.
R:

3. Os 5 URLs que mais causaram erro 404.
R:

4. Quantidade de erros 404 por dia.
R:

5. O total de bytes retornados.
R:

